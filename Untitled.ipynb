{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-10\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, state_size, action_size, is_continuous=False):\n",
    "        super().__init__()\n",
    "        self.is_continuous = is_continuous\n",
    "        \n",
    "        pi_model = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size)\n",
    "        )\n",
    "        \n",
    "        value_model = nn.Sequential(\n",
    "            nn.Linear(state_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "        \n",
    "        self.pi = pi_model\n",
    "        self.ve = value_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def getValue(self, x):\n",
    "        x = torch.tensor(x, dtype=DTYPE)\n",
    "        return self.ve(x)\n",
    "    \n",
    "    def getPi(self, x):\n",
    "        x = torch.tensor(x, dtype=DTYPE)\n",
    "        probs = self.pi(x)\n",
    "        return nn.Softmax(dim=-1)(probs) if not self.is_continuous else probs \n",
    "    \n",
    "    def setContinuous(self):\n",
    "        self.is_continuous = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, minl, maxl,alpha,sample_size=400):\n",
    "        self.max_length = maxl\n",
    "        self.length = 0\n",
    "        self.buffer = []\n",
    "        self.weights = []\n",
    "        self.alpha = alpha\n",
    "        self.min_length = minl\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "    def add(self, ep):\n",
    "        \n",
    "        weight = np.exp(self.alpha*np.array(ep['rewards']).sum())+eps\n",
    "#         if self.length>0 and weight < 0.75*np.min(self.weights) + 0.25 * np.mean(self.weights) and self.trainable(): return\n",
    "\n",
    "        self.buffer.append(ep)\n",
    "        self.length += 1\n",
    "\n",
    "        self.weights.append(weight)\n",
    "        \n",
    "        if self.length > self.max_length:\n",
    "            idx = np.random.randint(self.length)\n",
    "            del self.buffer[idx]\n",
    "            del self.weights[idx]\n",
    "            self.length-=1\n",
    "            \n",
    "    def sample(self):\n",
    "        wts = np.array(self.weights)\n",
    "        \n",
    "        sample_size = min(self.sample_size, self.length)\n",
    "        sample = np.random.choice(self.buffer,size=(sample_size,), p=wts/wts.sum(), replace=False)\n",
    "        out = sample if self.length>1 else [sample]\n",
    "        return out\n",
    "    \n",
    "    def trainable(self):\n",
    "        return self.length>self.min_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCL(object):\n",
    "    def __init__(self, env, replay_buffer, state_size, action_size, epoch=1000, off_policy_rate=20, pi_lr=7e-4, \n",
    "                 ve_lr=3.5e-4, entropy_tau=0.15, rollout_d=20, gamma=1):\n",
    "        self.epoch = epoch\n",
    "        self.state_size =  state_size\n",
    "        self.action_size = action_size\n",
    "        self.net = Net(self.state_size, self.action_size)\n",
    "        self.net.is_continuous = False\n",
    "        \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.env = env\n",
    "        \n",
    "        self.pi_optimiser = optim.Adam(self.net.pi.parameters(), lr=pi_lr)\n",
    "        self.ve_optimiser = optim.Adam(self.net.ve.parameters(), lr=ve_lr)\n",
    "        self.off_policy_rate = off_policy_rate\n",
    "        self.tau = entropy_tau\n",
    "        self.rollout_d = rollout_d\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def selectProb(self, pi, a):\n",
    "        a = np.eye(self.action_size, dtype=np.int32)[a]\n",
    "        pi_selected = torch.sum(pi * torch.tensor(a, dtype=DTYPE), dim=1)\n",
    "        return torch.log(pi_selected+eps)\n",
    "\n",
    "        \n",
    "    def optimise(self, episodes):\n",
    "        net = self.net\n",
    "        \n",
    "        self.pi_optimiser.zero_grad()\n",
    "        self.ve_optimiser.zero_grad()\n",
    "        \n",
    "#         print(\"before loop\", episodes)\n",
    "        for episode in episodes:\n",
    "            rollout_d = min(self.rollout_d, len(episode['states']))\n",
    "            ma = len(episode['states'])-rollout_d+1\n",
    "            \n",
    "            for i in range(ma):\n",
    "                states = episode['states'][i:i+rollout_d]\n",
    "                a = episode['actions'][i:i+rollout_d]  \n",
    "                R = episode['rewards'][i:i+rollout_d]\n",
    "\n",
    "                ve_init = net.getValue(states[0])\n",
    "                ve_end = net.getValue(states[-1])\n",
    "\n",
    "                pi_all_states = net.getPi(states).squeeze(0)\n",
    "                log_pi_selected = self.selectProb(pi_all_states, a)\n",
    "\n",
    "                discount = torch.tensor(self.gamma**np.arange(0,rollout_d))\n",
    "\n",
    "                c = -ve_init + self.gamma**rollout_d*ve_end\\\n",
    "                + torch.sum(discount*(torch.tensor(R) - self.tau*log_pi_selected))\n",
    "\n",
    "                loss = c**2\n",
    "                loss.backward()\n",
    "                \n",
    "#                 print(\"----------------------------\", loss)\n",
    "#         print(\"after loop\")\n",
    "        \n",
    "        self.ve_optimiser.step()\n",
    "        self.pi_optimiser.step()\n",
    "        \n",
    "    def getStateEnc(self,x):\n",
    "        return x\n",
    "    def getStateDec(self,x):\n",
    "        return x\n",
    "    def getActionEnc(self,a):\n",
    "        return a\n",
    "    def getActionDec(self,a):\n",
    "        return a\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        enc_state = self.getStateEnc(state)\n",
    "        \n",
    "        pi = self.net.getPi(enc_state).squeeze(0).detach().numpy()\n",
    "        return self.getActionDec(np.random.choice(np.arange(self.action_size), p=pi))\n",
    "    \n",
    "    def rollout(self, max_ep_length = -1):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        env = self.env\n",
    "        \n",
    "        state = env.reset()\n",
    "        is_terminated = False\n",
    "        timestep = 0\n",
    "        \n",
    "        while not is_terminated and timestep!=max_ep_length:\n",
    "            action = self.getAction(state)\n",
    "            next_state, reward, is_terminated, _ = env.step(action)\n",
    "            \n",
    "            states.append(self.getStateEnc(state))\n",
    "            actions.append(self.getActionEnc(action))\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            timestep += 1\n",
    "\n",
    "        \n",
    "        rewards = np.array(rewards)\n",
    "#         print(np.mean(rewards))\n",
    "#         rewards = (rewards - np.mean(rewards))\n",
    "        \n",
    "#         print(len(states))\n",
    "#         print(\"\")\n",
    "        return dict(\n",
    "            states = states,\n",
    "            actions = actions,\n",
    "            rewards = list(rewards)\n",
    "        )\n",
    "    \n",
    "    def train(self, max_ep_length=-1):\n",
    "        tot_rewards = []\n",
    "        for i in range(self.epoch):\n",
    "            episode = self.rollout(max_ep_length)        \n",
    "            self.optimise([episode])\n",
    "            \n",
    "            r = np.array(episode['rewards'])\n",
    "            tot_rewards.append(r.sum())\n",
    "            \n",
    "            if i>20: \n",
    "                print(\"I: {},    B:{}       R: {}                    \".format(i, self.replay_buffer.length, np.mean(tot_rewards[-20:])), end='\\r')\n",
    "            else:\n",
    "                print(\"I: {},    B:{}       R: {}                    \".format(i, self.replay_buffer.length, tot_rewards[-1]), end='\\r')\n",
    "                \n",
    "            self.replay_buffer.add(episode)\n",
    "            \n",
    "            ########off policy\n",
    "            if self.replay_buffer.trainable():\n",
    "                for _ in range(self.off_policy_rate):\n",
    "                    episodes = self.replay_buffer.sample()\n",
    "                    self.optimise(episodes)\n",
    "            \n",
    "#             print(i)\n",
    "#             for x in self.net.pi.parameters():\n",
    "#                 print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousPCL(PCL):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.net.setContinuous()\n",
    "        \n",
    "    def selectProb(self, params, a):\n",
    "        \"\"\"\n",
    "        params: [mean, var]\n",
    "        \"\"\"\n",
    "        \n",
    "        mean = params[:, 0]\n",
    "        std = params[:, 1]\n",
    "        a = torch.tensor(a, dtype=DTYPE)\n",
    "        \n",
    "        return -(a-mean)**2/(std**2+eps) - std  \n",
    "    \n",
    "    def getAction(self, state):\n",
    "        enc_state = self.getStateEnc(state)\n",
    "        params = self.net.getPi(enc_state).squeeze(0).detach()\n",
    "        action = Normal(params[0], params[1]).sample().numpy()\n",
    "        return np.array([action])\n",
    "    \n",
    "    def getActionEnc(self, action):\n",
    "        return torch.tensor(action, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyPCL(PCL):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def getStateEnc(self,x):\n",
    "        return np.array([x])\n",
    "    \n",
    "    def getStateDec(self,x):\n",
    "        return x[0]\n",
    "    \n",
    "    def getActionEnc(self,a):\n",
    "        return a[0]*6 +(1-a[1])*5+ a[2]\n",
    "    \n",
    "    def getActionDec(self,a):\n",
    "        return (a//6, 1-(a%6)//5, (a%6)%5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Copy-v0').env\n",
    "buffer = ReplayBuffer(32, 10000, 1)\n",
    "agent = CopyPCL(env, buffer, 1, 12, epoch=100000, off_policy_rate=20,entropy_tau=0.05, pi_lr=0.005, ve_lr=0.0025)\n",
    "# agent.getStateEnc = lambda x: np.eye(6)[int(x)]\n",
    "# agent.getStateDec = lambda x: np.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 145,    B:145       R: 0.85                     \r"
     ]
    }
   ],
   "source": [
    "agent.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 3)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.getAction(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
